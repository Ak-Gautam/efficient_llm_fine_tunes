{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI should be accessible to all\n",
        "GPU poor or GPU rich!"
      ],
      "metadata": {
        "id": "tJVVLvghqLMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and Inference Qwen-0.5B\n",
        "Here we just test run the raw model using vllm to get an idea of the performance and memory usage."
      ],
      "metadata": {
        "id": "HotRR8irqao0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1UAS83Dp_4s"
      },
      "outputs": [],
      "source": [
        "!pip install vllm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mport time\n",
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "def load_model(model_name, dtype):\n",
        "    \"\"\"\n",
        "    Load a language model and measure the time it takes to load.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): The name or path of the model to load.\n",
        "        dtype (str): The data type to use for the model.\n",
        "\n",
        "    Returns:\n",
        "        LLM: The loaded language model instance.\n",
        "    \"\"\"\n",
        "    loading_start = time.time()\n",
        "    llm = LLM(model=model_name, dtype=dtype)\n",
        "    loading_time = time.time() - loading_start\n",
        "    print(f\"--- Loading time: {loading_time:.2f} seconds ---\")\n",
        "    return llm\n",
        "\n",
        "def generate_text(llm, prompts, sampling_params):\n",
        "    \"\"\"\n",
        "    Generate text from the given prompts using the loaded language model.\n",
        "\n",
        "    Args:\n",
        "        llm (LLM): The loaded language model instance.\n",
        "        prompts (list): A list of prompts to generate text from.\n",
        "        sampling_params (SamplingParams): The sampling parameters to use for text generation.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of outputs containing the generated text for each prompt.\n",
        "    \"\"\"\n",
        "    generation_start = time.time()\n",
        "    outputs = llm.generate(prompts, sampling_params)\n",
        "    generation_time = time.time() - generation_start\n",
        "    print(f\"--- Generation time: {generation_time:.2f} seconds ---\")\n",
        "    return outputs\n",
        "\n",
        "def print_generated_text(outputs):\n",
        "    \"\"\"\n",
        "    Print the generated text from the outputs.\n",
        "\n",
        "    Args:\n",
        "        outputs (list): A list of outputs containing the generated text.\n",
        "    \"\"\"\n",
        "    for output in outputs:\n",
        "        generated_text = output.outputs[0].text\n",
        "        print(generated_text)\n",
        "        print('------')"
      ],
      "metadata": {
        "id": "oLCCrh4MtKX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "    \"Driving a manual car requires knowledge of\"\n",
        "]\n",
        "sampling_params = SamplingParams(temperature=0.7, top_p=0.8, top_k=20, max_tokens=150)\n",
        "\n",
        "llm = load_model(\"Qwen/Qwen1.5-0.5B\", \"half\")\n",
        "\n",
        "try:\n",
        "    outputs = generate_text(llm, prompts, sampling_params)\n",
        "    print_generated_text(outputs)"
      ],
      "metadata": {
        "id": "tbpnJCDdvmlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finally:\n",
        "    # Clean up the LLM instance and free resources.\n",
        "    llm.close()"
      ],
      "metadata": {
        "id": "Ol7sXx7OuMnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-Tuning\n",
        "Using the QloRA technique"
      ],
      "metadata": {
        "id": "nxyY-ocAxY99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade bitsandbytes transformers peft accelerate datasets trl"
      ],
      "metadata": {
        "id": "HX700sTYxYjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "\n",
        "def load_tokenizer(model_name):\n",
        "   \"\"\"\n",
        "   Load and configure the tokenizer for the given model.\n",
        "\n",
        "   Args:\n",
        "       model_name (str): The name of the pre-trained model.\n",
        "\n",
        "   Returns:\n",
        "       tokenizer (AutoTokenizer): The configured tokenizer instance.\n",
        "   \"\"\"\n",
        "   tokenizer = AutoTokenizer.from_pretrained(model_name, add_eos_token=True, use_fast=True)\n",
        "   tokenizer.pad_token = tokenizer.eos_token\n",
        "   tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "   tokenizer.padding_side = 'left'\n",
        "   return tokenizer\n",
        "\n",
        "\n",
        "def load_model(model_name, tokenizer):\n",
        "   \"\"\"\n",
        "   Load and configure the model for PEFT training.\n",
        "\n",
        "   Args:\n",
        "       model_name (str): The name of the pre-trained model.\n",
        "       tokenizer (AutoTokenizer): The tokenizer instance.\n",
        "\n",
        "   Returns:\n",
        "       model (AutoModelForCausalLM): The configured model instance.\n",
        "   \"\"\"\n",
        "   compute_dtype = getattr(torch, \"float16\")\n",
        "   bnb_config = BitsAndBytesConfig(\n",
        "       load_in_4bit=True,\n",
        "       bnb_4bit_quant_type=\"nf4\",\n",
        "       bnb_4bit_compute_dtype=compute_dtype,\n",
        "       bnb_4bit_use_double_quant=True,\n",
        "   )\n",
        "   model = AutoModelForCausalLM.from_pretrained(\n",
        "       model_name, quantization_config=bnb_config, device_map={\"\": 0}\n",
        "   )\n",
        "   model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "   # Configure the pad token in the model\n",
        "   model.config.pad_token_id = tokenizer.pad_token_id\n",
        "   model.config.use_cache = False  # Gradient checkpointing is used by default but not compatible with caching\n",
        "\n",
        "   return model\n",
        "\n",
        "\n",
        "def configure_peft_lora():\n",
        "   \"\"\"\n",
        "   Configure the PEFT LoRA (Low-Rank Adaptation) parameters.\n",
        "\n",
        "   Returns:\n",
        "       peft_config (LoraConfig): The configured PEFT LoRA instance.\n",
        "   \"\"\"\n",
        "   peft_config = LoraConfig(\n",
        "       lora_alpha=16,\n",
        "       lora_dropout=0.05,\n",
        "       r=16,\n",
        "       bias=\"none\",\n",
        "       task_type=\"CAUSAL_LM\",\n",
        "       target_modules=['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"]\n",
        "   )\n",
        "   return peft_config\n",
        "\n",
        "\n",
        "def configure_training_arguments():\n",
        "   \"\"\"\n",
        "   Configure the training arguments for the PEFT fine-tuning process.\n",
        "\n",
        "   Returns:\n",
        "       training_arguments (TrainingArguments): The configured training arguments instance.\n",
        "   \"\"\"\n",
        "   training_arguments = TrainingArguments(\n",
        "       output_dir=\"./drive/Mydrive/results_qlora\",\n",
        "       evaluation_strategy=\"steps\",\n",
        "       do_eval=True,\n",
        "       optim=\"paged_adamw_8bit\",\n",
        "       per_device_train_batch_size=8,\n",
        "       per_device_eval_batch_size=8,\n",
        "       log_level=\"debug\",\n",
        "       save_steps=50,\n",
        "       logging_steps=50,\n",
        "       learning_rate=2e-5,\n",
        "       eval_steps=50,\n",
        "       max_steps=300,\n",
        "       warmup_steps=30,\n",
        "       lr_scheduler_type=\"linear\",\n",
        "   )\n",
        "   return training_arguments"
      ],
      "metadata": {
        "id": "EY0xydnu2H2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model name\n",
        "model_name = \"Qwen/Qwen1.5-0.5B\"\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = load_tokenizer(model_name)\n",
        "\n",
        "# Load the dataset\n",
        "ds = load_dataset(\"timdettmers/openassistant-guanaco\")\n",
        "\n",
        "# Load and configure the model\n",
        "model = load_model(model_name, tokenizer)\n",
        "\n",
        "# Configure the PEFT LoRA\n",
        "peft_config = configure_peft_lora()\n",
        "\n",
        "# Configure the training arguments\n",
        "training_arguments = configure_training_arguments()\n",
        "\n",
        "# Create the trainer instance\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=ds['train'],\n",
        "    eval_dataset=ds['test'],\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=512,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        ")\n",
        "\n",
        "# Start the training process\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "DVRFp5A32IRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference the fine-tuned model"
      ],
      "metadata": {
        "id": "JpQr3-dh4ydz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "def load_model_and_tokenizer(output_dir):\n",
        "    \"\"\"\n",
        "    Load the model and tokenizer from the specified output directory.\n",
        "\n",
        "    Args:\n",
        "        output_dir (str): The path to the output directory containing the model and tokenizer files.\n",
        "\n",
        "    Returns:\n",
        "        model (AutoModelForCausalLM): The loaded language model.\n",
        "        tokenizer (AutoTokenizer): The loaded tokenizer.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
        "        model = AutoModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model and tokenizer: {e}\")\n",
        "        return None, None\n",
        "    return model, tokenizer\n",
        "\n",
        "def inference(model, tokenizer, messages):\n",
        "    \"\"\"\n",
        "    Perform inference using the loaded model and tokenizer.\n",
        "\n",
        "    Args:\n",
        "        model (AutoModelForCausalLM): The loaded language model.\n",
        "        tokenizer (AutoTokenizer): The loaded tokenizer.\n",
        "        messages (list): A list of dictionaries containing the conversation history.\n",
        "\n",
        "    Returns:\n",
        "        output (str): The generated response from the model.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        input_ids = tokenizer.apply_chat_template(messages, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_new_tokens=256,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_k=50,\n",
        "            top_p=0.95\n",
        "        )\n",
        "        output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "    except Exception as e:\n",
        "        print(f\"Error during inference: {e}\")\n",
        "        output = \"Sorry, I encountered an error and couldn't generate a response.\"\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "-CYEVP904iTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"/content/drive/Mydrive/results_qlora/checkpoint-300\"\n",
        "\n",
        "model, tokenizer = load_model_and_tokenizer(output_dir)\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
        "    },\n",
        "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
        "]\n",
        "\n",
        "output = inference(model, tokenizer, messages)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "Jzj-RWGE2kUH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
